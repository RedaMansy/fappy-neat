{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import math\n",
                "import numpy as np\n",
                "import tensorflow as tf \n",
                "import sys\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "import flappy_bird_gym\n",
                "import yaml\n",
                "import pygame\n",
                "\n",
                "LOCAL_PATH=os.path.abspath('')\n",
                "MAIN_PATH=LOCAL_PATH+''\n",
                "\n",
                "CONF_PATH=MAIN_PATH+\"/CONF\"\n",
                "DEMO_PATH=MAIN_PATH+\"/DEMO\"\n",
                "DOC_PATH=MAIN_PATH+\"/DOC\"\n",
                "ENV_PATH=MAIN_PATH+\"/LIB/GAME\"\n",
                "LIB_PATH=MAIN_PATH+\"/LIB\"\n",
                "LOG_PATH=MAIN_PATH+\"/LOG\"\n",
                "IMG_PATH=MAIN_PATH+\"/Images\"\n",
                "POLICIES_PATH=MAIN_PATH+\"/POLICIES\"\n",
                "TEMP_PATH=MAIN_PATH+\"/TEMP\"\n",
                "TRN_PATH=MAIN_PATH+\"/LIB/TRAINING\"\n",
                "\n",
                "sys.path.insert(1, ENV_PATH)\n",
                "sys.path.insert(1, TRN_PATH)\n",
                "sys.path.insert(1, LIB_PATH)\n",
                "\n",
                "class DQN(tf.keras.Model):\n",
                "    def __init__(self,n_actions=4,fc1_dims=512):\n",
                "        super().__init__()\n",
                "        self.n_actions=n_actions\n",
                "        self.fc1_dims=fc1_dims\n",
                "        self.d0 = tf.keras.layers.Flatten()\n",
                "        self.d1 = tf.keras.layers.Dense(self.fc1_dims,activation='relu')\n",
                "        self.q_values = tf.keras.layers.Dense(self.n_actions,activation='linear')\n",
                "        \n",
                "    def call(self, state):\n",
                "        x = tf.convert_to_tensor(state)\n",
                "        x = self.d0(x)\n",
                "        x = self.d1(x)\n",
                "        x = self.q_values(x)\n",
                "        return x\n",
                "    \n",
                "class ReplayBuffer():\n",
                "    def __init__(self, buffer_size=50000,input_shape=[0,0,0,0]):\n",
                "        self.buffer_size = buffer_size\n",
                "        self.input_shape=input_shape\n",
                "        self.state_mem = np.zeros((self.buffer_size,*(self.input_shape)), dtype=np.float32)\n",
                "        self.action_mem = np.zeros((self.buffer_size), dtype=np.int32)\n",
                "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
                "        self.next_state_mem = np.zeros((self.buffer_size,*(self.input_shape)), dtype=np.float32)\n",
                "        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool)\n",
                "        self.pointer = 0\n",
                "\n",
                "    def add_exp(self, state, action, reward, next_state, done):\n",
                "        idx  = self.pointer % self.buffer_size \n",
                "        self.state_mem[idx] = state\n",
                "        self.action_mem[idx] = action\n",
                "        self.reward_mem[idx] = reward\n",
                "        self.next_state_mem[idx] = next_state\n",
                "        self.done_mem[idx] = 1 - int(done)\n",
                "        self.pointer += 1\n",
                "\n",
                "    def sample_exp(self, batch_size= 64):\n",
                "        max_mem = min(self.pointer, self.buffer_size)\n",
                "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
                "        states = self.state_mem[batch]\n",
                "        actions = self.action_mem[batch]\n",
                "        rewards = self.reward_mem[batch]\n",
                "        next_states = self.next_state_mem[batch]\n",
                "        dones = self.done_mem[batch]\n",
                "\n",
                "        return states, actions, rewards, next_states, dones\n",
                "\n",
                "class SimpleQAgent():\n",
                "    def __init__(self, gamma=0.99, replace=100, lr=0.05,n_actions=2,input_shape=[0,0,0,0],buffer_size= 50000,batch_size=64):\n",
                "        self.buffer_size = buffer_size\n",
                "        self.input_shape=input_shape\n",
                "        self.n_actions=n_actions\n",
                "        self.gamma = gamma\n",
                "        self.epsilon = 1.0\n",
                "        self.min_epsilon = 0.01\n",
                "        self.epsilon_decay = 1e-3\n",
                "        self.replace = replace\n",
                "        self.trainstep = 0\n",
                "        self.memory = ReplayBuffer(input_shape=self.input_shape,buffer_size=self.buffer_size)\n",
                "        self.batch_size = batch_size\n",
                "        self.q_net = DQN()\n",
                "        self.target_net = DQN()\n",
                "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
                "        self.q_net.compile(loss='huber', optimizer=opt)\n",
                "        self.target_net.compile(loss='huber', optimizer=opt)\n",
                "\n",
                "    def update_mem(self, state, action, reward, next_state, done):\n",
                "        self.memory.add_exp(state, action, reward, next_state, done)\n",
                "\n",
                "    def update_target(self):\n",
                "        self.target_net.set_weights(self.q_net.get_weights())  \n",
                "\n",
                "    def act(self, state):\n",
                "        if np.random.rand() <= self.epsilon:\n",
                "            return np.random.choice([i for i in range(self.n_actions)],p=[0.93,0.07]),False\n",
                "        else:\n",
                "            state=tf.convert_to_tensor([state.flatten()],dtype=tf.float32)\n",
                "            actions = self.q_net(state)\n",
                "            action = np.argmax(actions)\n",
                "            return action,True\n",
                "\n",
                "    # def update_mem(self, state, action, reward, next_state, done):\n",
                "    #     self.memory.add_exp(state, action, reward, next_state, done)\n",
                "\n",
                "    def update_target(self):\n",
                "        self.target_net.set_weights(self.q_net.get_weights())   \n",
                "\n",
                "    def update_epsilon(self):\n",
                "        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
                "        return self.epsilon\n",
                "\n",
                "    def train_simple(self):\n",
                "        if self.memory.pointer < self.batch_size:\n",
                "            return 0,0\n",
                "        with tf.GradientTape() as tape:\n",
                "            states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
                "            target = self.q_net(states)\n",
                "            q_next = tf.math.reduce_max(self.target_net(next_states), axis=1,keepdims=True).numpy()\n",
                "            q_target = np.copy(target)  #optional  \n",
                "            maxq=tf.reduce_mean(tf.math.reduce_max(target, axis=1,keepdims=True).numpy())\n",
                "            for idx, terminal in enumerate(dones):\n",
                "                if terminal:\n",
                "                    q_next[idx]=0.0\n",
                "                q_target[idx,actions[idx]]=rewards[idx]+self.gamma*q_next[idx]\n",
                "            loss=tf.reduce_mean((q_target-target)**2)\n",
                "        gradient= tape.gradient(loss,self.q_net.trainable_variables)\n",
                "        self.q_net.optimizer.apply_gradients(zip(gradient,self.q_net.trainable_variables))\n",
                "        self.trainstep += 1\n",
                "        return float(maxq.numpy()),float(1)\n",
                "\n",
                "class DQL:\n",
                "    def __init__(self):\n",
                "        print(\"Flappy Bird Training\")\n",
                "        pygame.init()\n",
                "        pygame.display.set_caption(\"Flappy Bird\")\n",
                "        self.experiment_name=\"dql\"\n",
                "        self.n_actions,self.max_epochs,self.max_episodes,self.max_steps,self.max_e,self.min_e,self.e_decay,self.w_updates,self.t_updates,self.t_steps,self.mini_batch_size,self.max_queue_length=self.load_params(CONF_PATH)\n",
                "        self.environment=flappy_bird_gym.make(\"FlappyBird-v0\",pipe_gap = 150)\n",
                "        s = self.environment.reset()\n",
                "\n",
                "        self.environment.step(0)\n",
                "        self.agente=SimpleQAgent(n_actions=2,input_shape=s.shape,buffer_size=self.max_queue_length,batch_size=self.mini_batch_size)\n",
                "        self.state=tf.convert_to_tensor([s.flatten()],dtype=tf.float32)\n",
                "        self.agente.q_net(self.state)\n",
                "        self.agente.target_net(self.state)\n",
                "        self.rtrn = []\n",
                "        self.avg_r = []\n",
                "        self.loss = []\n",
                "        self.loss_avg = []\n",
                "        self.max_q_net = []\n",
                "        self.max_q_net_avg = []\n",
                "        self.environment.dt = 30.0/1000.0\n",
                "\n",
                "    def episode_training(self,mode):\n",
                "        normal=False\n",
                "        if mode==\"random\":\n",
                "            print(\"Episode training random\")\n",
                "            name=\"random_episodes\"\n",
                "            epsilon=1.0\n",
                "        else:\n",
                "            print(\"Episode training \",self.experiment_name)\n",
                "            name=self.experiment_name+\"_episodes\"\n",
                "            normal=True\n",
                "\n",
                "        episode=0\n",
                "        weight_updates=0\n",
                "        step_num=0\n",
                "        rwd=0\n",
                "        rwd_nn=0\n",
                "        avg_r_nn=[0]\n",
                "        avg_r=[0]\n",
                "        eoe_avg_r=[0]\n",
                "        Rtrn=[0]\n",
                "        eoe_Rtrn=[0]\n",
                "        Rtrn_nn=[0]\n",
                "\n",
                "        biased=0\n",
                "        while episode<self.max_episodes:\n",
                "            episode+=1\n",
                "            d=False\n",
                "            expert_action=False\n",
                "            s=self.environment.reset()\n",
                "            action=self.agente.act(s)\n",
                "            Rtrn.append(rwd)\n",
                "            eoe_Rtrn.append(rwd)\n",
                "            Rtrn_nn.append(rwd_nn)\n",
                "            # eoe_Rtrn_nn.append(rwd_nn)\n",
                "            rwd=0\n",
                "            rwd_nn=0\n",
                "            last_score=0\n",
                "            while not d:\n",
                "                #time.sleep(0.1)\n",
                "                step_num+=1\n",
                "                if step_num==self.max_steps:\n",
                "                    break\n",
                "                for event in pygame.event.get():\n",
                "                    if event.type == pygame.QUIT:\n",
                "                        pygame.quit()\n",
                "                if normal:\n",
                "                    self.agente.epsilon= self.min_e + (self.max_e - self.min_e) * np.exp(-self.e_decay*(episode))\n",
                "                    action,nn=self.agente.act(s)\n",
                "                else:\n",
                "                    self.agente.epsilon=1.0\n",
                "                    action,nn=self.agente.act(s)\n",
                "\n",
                "                s_, r, d, info = self.environment.step(action)\n",
                "\n",
                "                if (abs(1/(math.e**(s_[1]+0.0)))**2)>1.1 or (abs(1/(math.e**(s_[1]+0.0)))**2)<0.9:\n",
                "                    r=-1-abs(1-(1/(math.e**(s_[1].astype(float)+0.08))))\n",
                "                else:\n",
                "                    r=1\n",
                "\n",
                "                r=float(r)\n",
                "\n",
                "                if info['score']!=last_score:\n",
                "                    biased=9\n",
                "                    last_score=info['score']\n",
                "                else:\n",
                "                    biased=0\n",
                "                r+=biased\n",
                "                #self.environment.render()\n",
                "                rwd+=r\n",
                "\n",
                "                if nn:\n",
                "                    rwd_nn+=r\n",
                "\n",
                "                state=tf.convert_to_tensor([s],dtype=tf.float32)\n",
                "                state_=tf.convert_to_tensor([s_],dtype=tf.float32)\n",
                "                self.agente.update_mem(state, action, r, state_, d)\n",
                "\n",
                "                if step_num%self.t_steps==0 and step_num>1:\n",
                "                    if normal:\n",
                "                        q_max,loss_t=self.agente.train_simple()\n",
                "                        weight_updates+=1\n",
                "                        # loss.append(float(loss_t))\n",
                "                        # maxq.append(float(q_max))\n",
                "                        # eoe_loss.append(float(loss_t))\n",
                "                        # eoe_maxq.append(float(q_max))\n",
                "                s=s_\n",
                "\n",
                "                if normal:\n",
                "                    if weight_updates%self.t_updates==0 and  weight_updates>0:\n",
                "                        self.agente.target_net.set_weights(self.agente.q_net.get_weights())\n",
                "            print(\"[\",name,\"]Training Episode \",episode,\" of \",self.max_episodes)\n",
                "\n",
                "            if normal:\n",
                "                avg_r_nn.append(sum(Rtrn_nn)/len(Rtrn_nn))\n",
                "\n",
                "            eoe_avg_r.append(sum(eoe_Rtrn)/len(eoe_Rtrn))\n",
                "            avg_r.append(sum(Rtrn)/len(Rtrn))\n",
                "            self.save_data(TEMP_PATH,name,avg_r,eoe_avg_r,avg_r_nn)\n",
                "\n",
                "        self.agente.q_net.save(POLICIES_PATH+\"/\"+name)\n",
                "\n",
                "    def load_params(self,path):\n",
                "        with open(r''+path+\"/\"+self.experiment_name+\"_conf.yaml\") as parameters:\n",
                "            config_list = yaml.safe_load(parameters)\n",
                "        #HYPERPARAMETERS\n",
                "        mini_batch_size=config_list['training']['mini_batch_size']\n",
                "        n_actions=config_list['training']['num_actions']\n",
                "        learning_rate=config_list['training']['learning_rate']\n",
                "        max_episodes=config_list['training']['num_episodes']\n",
                "        max_epochs=config_list['training']['num_epochs']\n",
                "        max_e=config_list['epsilon']['max_epsilon']\n",
                "        max_steps=config_list['rl']['max_steps_per_episode']\n",
                "        max_queue_length=config_list['rl']['max_queue_length']\n",
                "        min_e=config_list['epsilon']['min_epsilon']\n",
                "        e_decay=config_list['epsilon']['decay_epsilon']\n",
                "        w_updates=config_list['training']['weight_updates']\n",
                "        t_steps=config_list['training']['train_steps']\n",
                "        t_updates=config_list['rl']['target_update_episodes']\n",
                "        return n_actions,max_epochs,max_episodes,max_steps,max_e,min_e,e_decay,w_updates,t_updates,t_steps,mini_batch_size,max_queue_length    \n",
                "           \n",
                "    def save_data(self,path,name,avg_r,avg_maxq, avg_loss):\n",
                "        with open(r''+path+\"/\"+name+'.yaml', 'w') as outfile:\n",
                "                           data={'avr_r':avg_r}\n",
                "                           yaml.dump(data, outfile, default_flow_style=False)\n",
                "        return\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    print(\"Training FlappyBirds environment with DQL\")\n",
                "    env=DQL()\n",
                "    #env.episode_training(\"random\")\n",
                "    env.episode_training(\"qdl\")\n",
                "    pygame.quit()"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.10 64-bit ('universe': conda)"
        },
        "interpreter": {
            "hash": "7163d73825b37174225d42fc02a5c1e2ba15e9c0d0bffb7b2527665ce0181df7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}